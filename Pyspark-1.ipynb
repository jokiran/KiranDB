{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977a8399-be04-4442-bbda-b580633622b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create data frame --\n",
    "data=[(1,'kiran',3000),(2,'rahul',5000)]\n",
    "schema=['id','name','salary']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "#Read_Files\n",
    "df=spark.read.format('csv').option('header',True).option('delimiter',',').load('/Volumes/kirandb/sample_files/kiran/Department1.csv')\n",
    "df.show()\n",
    "\n",
    "df=spark.read.format('json').option('multiline',True).load('/Volumes/kirandb/sample_files/kiran/EMPJSON (1).json')\n",
    "df.show()\n",
    "\n",
    "df=spark.read.format('parquet').load('/Volumes/kirandb/sample_files/kiran/NYCTripSmall.parquet')\n",
    "df.show()\n",
    "\n",
    "#Write_Files\n",
    "df1=df.write.option('delimiter',',').option('header',True).csv('/Volumes/kirandb/sample_files/kiran/Output.csv')\n",
    "df2=df.write.option('multiline',True).json('/Volumes/kirandb/sample_files/kiran/Output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c141ad3-fbd7-4191-a9d9-51d319080646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#With column operations --\n",
    "from pyspark.sql.types import cast \n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "data=[(1,'kiran',3000),(2,'rahul',5000)]\n",
    "schema=['id','name','salary']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "df1=df.withColumn('salary',df.salary.cast('int'))\n",
    "df2=df.withColumn('salary',df.salary*2)\n",
    "df2.show()\n",
    "df3=df.withColumn('city',lit('pune'))\n",
    "df3.show()\n",
    "df4=df.withColumnRenamed('salary','new_salary')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57449f3c-a4f6-42ce-baf0-24d5ffa61354",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764849526386}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#applying_functions\n",
    "from pyspark.sql.functions import array,explode,array_contains,split\n",
    "from pyspark.sql.types import ArrayType,StringType,IntegerType,StructType,StructField\n",
    "data=[(1,'kiran',3000,['aws','sql'],'pani,puri','red','blue'),(2,'rahul',4000,['adf','pyspark'],'dahi,bhalla','green','orange')]\n",
    "schema=StructType([StructField('id',IntegerType()),StructField('name',StringType()),StructField('salary',IntegerType()),StructField('skills',ArrayType(StringType())),StructField('food',StringType()),StructField('color1',StringType()),StructField('color2',StringType())])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "df1=df.withColumn('arrayccheck',array_contains('skills','aws'))\n",
    "display(df1)\n",
    "\n",
    "df2=df.withColumn('colors',array(df.color1,df.color2))\n",
    "display(df2)\n",
    "\n",
    "df3=df.withColumn('explodcol',explode(df.skills))\n",
    "display(df3)\n",
    "\n",
    "df4=df.withColumn('food',split(df.food,','))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06105173-3c89-4b5b-95b5-01619b771ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop and fill null\n",
    "data=[(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(3,None,6000,399,['adf','python'],'veg,chilli','chennai','pondi')]\n",
    "schema=['id','name','salary','bonus','skills','food','city1','city2']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.dropna()\n",
    "df1.show()\n",
    "df2=df.fillna('unknown')\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7e1ceb-5f10-4f24-a413-6261ea38ce72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drop duplicates \n",
    "data=[(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(3,None,6000,399,['adf','python'],'veg,chilli','chennai','pondi'),(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(6,'Krish',6000,399,['adf','python'],'veg,chilli','chennai','pondi')]\n",
    "schema=['id','name','salary','bonus','skills','food','city1','city2']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.distinct().show()\n",
    "df2=df.dropDuplicates().show()\n",
    "df3=df.dropDuplicates(['salary']).show()\n",
    "\n",
    "#Drop duplicates from multiple df's\n",
    "data1=[(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(3,None,6000,399,['adf','python'],'veg,chilli','chennai','pondi'),(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(6,'Krish',6000,399,['adf','python'],'veg,chilli','chennai','pondi')]\n",
    "schema1=['id','name','salary','bonus','skills','food','city1','city2']\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "display(df1)\n",
    "data2=[(1,'kiran',3000,599,['aws','pyspark'],'vada,pav','pune','mumbai'),(None,'rahul',4000,499,['gcp','sql'],'chicken,burger','delhi','gurgao'),(3,None,6000,399,['adf','python'],'veg,chilli','chennai','pondi')]\n",
    "schema2=['id','name','salary','bonus','skills','food','city1','city2']\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "display(df2)\n",
    "\n",
    "newdf=df1.union(df2)\n",
    "newdf.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ccca44-2e14-4d3a-880b-e7d32999bf4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UDF-User defined functions-We write our own logic into the python function and register it with the sql which can be reused again on df is called UDF.\n",
    "\n",
    "data=[(1,'kiran',2000,200),(2,'rahul',3000,300)]\n",
    "schema=['id','name','salary','bonus']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "def totalpay(s,b):\n",
    "    return (s+b)\n",
    "df1=df.select('*',totalpay(df.salary,df.bonus).alias('pay'))\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bb0e68-659e-4928-8518-3e9e0547a52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#createOrReplace_Tempview\n",
    "data=[(1,'kiran',2000,200),(2,'rahul',3000,300)]\n",
    "schema=['id','name','salary','bonus']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.createOrReplaceTempView('emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ab884c-b5fa-497d-9f94-4458397d8016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e8e593-62ec-4063-ab59-a24aa3f53fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Joines in pyspark\n",
    "\n",
    "data1=[(1,'kiran','india'),(2,'novak','russia'),(3,'phleps','usa'),(4,'steve','aus'),(5,'dilshan','srilanka'),(6,'sherpa','nepal'),(7,'federe','swiss')]\n",
    "schema1=['cid','cname','country']\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "data2=[(11307,2,'2023-12-25'),(11308,5,'2023-12-20'),(11309,6,'2023-12-15'),(11310,8,'2023-12-10')]\n",
    "schema2=['orderid','cid','orderdate']\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "df1.show()\n",
    "df2.show()\n",
    "df1.join(df2,df1.cid==df2.cid,'inner').show()\n",
    "df1.join(df2,df1.cid==df2.cid,'left').show()\n",
    "df1.join(df2,df1.cid==df2.cid,'right').show()\n",
    "df1.join(df2,df1.cid==df2.cid,'full').show()\n",
    "df1.join(df2,df1.cid==df2.cid,'leftanti').show()\n",
    "df1.join(df2,df1.cid==df2.cid,'leftsemi').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686c4596-57c1-449a-91f1-1cb64f2cd0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Window_functions \n",
    "from pyspark.sql.functions import row_number,rank,dense_rank,lead,lag,ntile\n",
    "from pyspark.sql.window import Window\n",
    "window=Window.partitionBy('dept').orderBy('salary')\n",
    "data=[(1,'kiran','male',3000,'IT'),(2,'rahul','male',5000,'IT'),(3,'rajendra','male',7000,'IT'),(4,'sushant','male',3500,'Mech'),(5,'akshay','male',11000,'Mech'),(6,'rohini','female',2500,'sales'),(7,'samiksha','female',5500,'sales'),(8,'sunita','female',1500,'Instru'),(9,'anita','female',9500,'Admin'),(10,'soni','female',10000,'Admin')]\n",
    "schema=['id','name','gender','salary','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.withColumn('rn',row_number().over(window)).withColumn('rnk',rank().over(window)).withColumn('dnk',dense_rank().over(window)).withColumn('next_salary',lead('salary').over(window)).withColumn('pre_salary',lag('salary').over(window)).withColumn('range',ntile(2).over(window))\n",
    "df1.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4628a1-37db-443f-ac04-226a2798163b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aggregate_Functions\n",
    "from pyspark.sql.functions import *\n",
    "data=[(1,'kiran','male',3000,'IT'),(2,'rahul','male',5000,'IT'),(3,'rajendra','male',7000,'IT'),(4,'sushant','male',3500,'Mech'),(5,'akshay','male',11000,'Mech'),(6,'rohini','female',2500,'sales'),(7,'samiksha','female',5500,'sales'),(8,'sunita','female',1500,'Instru'),(9,'anita','female',9500,'Admin'),(10,'soni','female',10000,'Admin')]\n",
    "schema=['id','name','gender','salary','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.select(max(df.salary)).show()\n",
    "df2=df.select(min(df.salary)).show()\n",
    "df3=df.select(sum(df.salary)).show()\n",
    "df4=df.select(mean(df.salary)).show()\n",
    "df5=df.select(count(df.salary)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e148f766-50f7-4d9a-9265-72d3b21120e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sort and orderBy\n",
    "data=[(1,'kiran','male',3000,'IT'),(2,'rahul','male',5000,'IT'),(3,'rajendra','male',7000,'IT'),(4,'sushant','male',3500,'Mech'),(5,'akshay','male',11000,'Mech'),(6,'rohini','female',2500,'sales'),(7,'samiksha','female',5500,'sales'),(8,'sunita','female',1500,'Instru'),(9,'anita','female',9500,'Admin'),(10,'soni','female',10000,'Admin')]\n",
    "schema=['id','name','gender','salary','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "df1=df.sort(df.salary).show()\n",
    "df2=df.orderBy(df.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9cbf7bf-bd68-4068-b82d-5ee75ab5ea52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Operations on dictionary\n",
    "from pyspark.sql.functions import map_keys,map_values,explode\n",
    "from pyspark.sql.types import StructField,StructType,ArrayType,MapType,StringType\n",
    "data=[('kiran',{'hair':'black','eye':'blue'}),('rahul',{'hair':'grey','eye':'brown'})]\n",
    "schema=StructType([StructField('name',StringType()),StructField('props',MapType(StringType(),StringType()))])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df1=df.withColumn('eye',df.props.getItem('eye')).show()\n",
    "df2=df.withColumn('keys',map_keys(df.props)).show()\n",
    "df3=df.withColumn('values',map_values(df.props)).show()\n",
    "df4=df.select('name','props',explode(df.props)).show()\n",
    "\n",
    "df5=df.filter(df.name.like('_a%')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e9c927-6749-41a5-ad41-3488dae7ee06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pivot-It is rotating data from one column to multiple columns \n",
    "data=[(1,'kiran','male',3000,'IT'),(2,'rahul','male',5000,'IT'),(3,'rajendra','male',7000,'IT'),(4,'sushant','male',3500,'Mech'),(5,'akshay','male',11000,'Mech'),(6,'rohini','female',2500,'sales'),(7,'samiksha','female',5500,'sales'),(8,'sunita','female',1500,'Instru'),(9,'anita','female',9500,'Admin'),(10,'soni','female',10000,'Admin')]\n",
    "schema=['id','name','gender','salary','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "df1=df.groupBy('dept').pivot('gender').count().show()\n",
    "df2=df.groupBy('dept').pivot('gender',['male']).count().show()\n",
    "\n",
    "#Unpivot-it rotates the data from column into multiple rows \n",
    "from pyspark.sql.functions import expr\n",
    "df1=df.select('dept',expr(\"stack(2,'male','m','female','f')as (gender,count)\")).show()\n",
    "\n",
    "#collect-collect-It retrieves all elements in the dataframe as an array for the row type of a driver node.\n",
    "listRows=df.collect()\n",
    "print(listRows)\n",
    "print(listRows[0])\n",
    "print(listRows[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93e03fb-074c-4ca7-bbc1-8461cbc9fe34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To use the transform function \n",
    "from pyspark.sql.functions import upper \n",
    "data=[(1,'kiran','male',3000,'IT'),(2,'rahul','male',5000,'IT'),(3,'rajendra','male',7000,'IT'),(4,'sushant','male',3500,'Mech'),(5,'akshay','male',11000,'Mech'),(6,'rohini','female',2500,'sales'),(7,'samiksha','female',5500,'sales'),(8,'sunita','female',1500,'Instru'),(9,'anita','female',9500,'Admin'),(10,'soni','female',10000,'Admin')]\n",
    "schema=['id','name','gender','salary','dept']\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "def upperthename(df):\n",
    "    return df.withColumn('name',upper(df.name))\n",
    "def doublethesalary(df):\n",
    "    return df.withColumn('salary',df.salary*2)\n",
    "df1=df.transform(upperthename).transform(doublethesalary)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08206395-5603-4557-9f2c-bbef3d81bc61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Map()-It is RDD transformation used to apply functions(lambda) on every element of rdd and returns the new rdd.\n",
    "data=[('kiran','mane'),('rahul','shelke')]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "df=spark.createDataFrame(data,['fn','ln'])\n",
    "rdd1=rdd.map(lambda x:x+(x[0]+' '+x[1],))\n",
    "df1=rdd1.toDF(['fn','ln','fullname'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a845317-b37a-4fe0-90a0-f4746cd3ad80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#flatMap():It is the transformation operation that flattens the rdd/df..after applying the functions on every element and returns the new pyspark RDD.\n",
    "#In df's it is used to flatten the array.\n",
    "\n",
    "data=['kiran mane','Rahul shelke']\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for item in rdd.collect():\n",
    "    print(item)\n",
    "\n",
    "rdd1=rdd.flatMap(lambda x:x.split(' '))\n",
    "for item in rdd1.collect():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967f6f8e-4174-489b-9c06-5ae81b8988a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8398750725046434,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
